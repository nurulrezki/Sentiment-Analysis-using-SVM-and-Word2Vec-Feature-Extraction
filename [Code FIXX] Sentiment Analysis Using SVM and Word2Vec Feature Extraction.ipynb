{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **IMPORT DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ3YK-T03mnB",
        "outputId": "55688828-fcd5-49a8-eeea-8e37f4340f93"
      },
      "outputs": [],
      "source": [
        "# Importing Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Train Data\n",
        "df =  pd.read_excel('your excel dataset')\n",
        "print(\"Data shape:\",df.shape)\n",
        "NegatifDf = df[df[\"label\"]==\"negative\"].drop(\"label\",axis=1)\n",
        "NegatifDf[\"label\"] = 0\n",
        "\n",
        "PositifDf = df[df[\"label\"]==\"positive\"].drop(\"label\",axis=1)\n",
        "PositifDf[\"label\"] = 1\n",
        "\n",
        "df = pd.concat([NegatifDf,PositifDf],ignore_index=True)\n",
        "print(\"Final total dataset:\",df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **PRE-PROCESSING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8u4FXe65fge",
        "outputId": "87e6c875-ea4c-4cf1-c546-a37de0539a67"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGpqcf9W5ksR",
        "outputId": "3266c664-5549-4a51-b6d9-172f8d195d11"
      },
      "outputs": [],
      "source": [
        "stop=set(stopwords.words(\"indonesian\"))\n",
        "stop_bow = set(stopwords.words(\"indonesian\"))\n",
        "stop_bow.discard(\"tidak\")\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "text=df[\"tweet\"]\n",
        "cleaned_text_bow=[]\n",
        "cleaned_text=[]\n",
        "for line in text:\n",
        "    tags = re.compile(\"^@[a-zA-Z_]*\")\n",
        "    line = re.sub(tags,\" \",line)\n",
        "    hashtags = re.compile(\"#|\\*\")\n",
        "    extraCharacters = re.compile(\"[^a-zA-Z]\")\n",
        "    line = re.sub(extraCharacters,\" \",line)\n",
        "\n",
        "    filtered_words=[]\n",
        "    filtered_words_bow=\"\"\n",
        "    for word in line.split():\n",
        "        word=word.lower()\n",
        "        if(word not in stop):\n",
        "            word = stemmer.stem(word)\n",
        "            filtered_words.append(word)\n",
        "        if(word not in stop_bow):\n",
        "            word = stemmer.stem(word)\n",
        "            filtered_words_bow+=\" \"+word\n",
        "    cleaned_text.append(filtered_words)\n",
        "    cleaned_text_bow.append(filtered_words_bow)\n",
        "            \n",
        "data_bow = pd.DataFrame(data=cleaned_text_bow,columns=[\"tweet\"])\n",
        "data_bow[\"label\"] = df[\"label\"]\n",
        "df[\"tweet\"]=cleaned_text\n",
        "print(df.head(10))\n",
        "print()\n",
        "print(data_bow.head(10))\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **WORD2VEC FEATURE EXTRACTION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Word2Vec Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGWVwCJV6Cmk"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMviLC2T62bK"
      },
      "outputs": [],
      "source": [
        "#Train on text data\n",
        "list_of_sent = df[\"tweet\"]\n",
        "w2v_model=gensim.models.Word2Vec(list_of_sent, window=5, vector_size=100, workers=4, sg=1, negative=17) #size = ukuran vector yg diinginkan, default cbow\n",
        "out_weights = w2v_model.syn1neg\n",
        "#out_weights\n",
        "# corpus/vocab ambil dr kata yg ada di data trainw2v_words=list(w2v_model.wv.index_to_key) \n",
        "w2v_words=list(w2v_model.wv.index_to_key) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vectorize train text data\n",
        "listof_sent_vec=[]\n",
        "for sent in tqdm(list_of_sent): \n",
        "    sent_vec = np.zeros(100) \n",
        "    cnt_words =0; \n",
        "    for word in sent: \n",
        "        if word in w2v_words:\n",
        "            vec = w2v_model.wv[word]\n",
        "            sent_vec += vec\n",
        "            cnt_words += 1\n",
        "    if cnt_words != 0:\n",
        "        sent_vec /= cnt_words\n",
        "    listof_sent_vec.append(sent_vec)\n",
        "    \n",
        "Label = df[\"label\"]\n",
        "list_col=tuple(range(100))\n",
        "Scaler = StandardScaler()\n",
        "data_vec = Scaler.fit_transform(listof_sent_vec)\n",
        "W2v_data=pd.DataFrame(data=data_vec, columns=list_col)\n",
        "W2v_data[\"label\"] = Label\n",
        "#print(W2v_data.head(10))\n",
        "#print(W2v_data.shape) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mapping X dan y Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF_VOP277Luu",
        "outputId": "1b486bbc-9b81-4fb1-f80d-fb02606dfd70"
      },
      "outputs": [],
      "source": [
        "X = W2v_data.drop(\"label\",axis=1).to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = W2v_data[\"label\"].to_numpy()\n",
        "y[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Jalankan ini untuk dapatkan indeks awal pada hasil splitting data, tapi hapus untuk dapatkan idnex data hasil split baru cocokkan\n",
        "y = pd.Series(Label)\n",
        "y[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **SVM CLASSIFICATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import model_selection, svm\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.sparse import csr_matrix as matrixTransform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Worked Code for reducted and not-reducted Data - Kfold CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#FIXX CODE\n",
        "# # Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "# Importing the dataset\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=0)\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "# Fitting Kernel SVM to the Training set\n",
        "from sklearn.svm import SVC\n",
        "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
        "classifier.fit(X_train, y_train)\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "from sklearn.model_selection import cross_val_score\n",
        "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get train proportion\n",
        "import numpy\n",
        "unique, counts = numpy.unique(y_test, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#visualisasikan confusion matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import metrics\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [0, 1])\n",
        "cm_display.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FN = cm[1][0]\n",
        "FP = cm [0][1]\n",
        "TP = cm[1][1]\n",
        "TN = cm[0][0]\n",
        "(TP, TN, FP, FN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get index data of confusion matrix element\n",
        "FP = cm [0][1]\n",
        "FP_rows = []\n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "    if y_pred[i] == 1 and y_test[i] == 0:\n",
        "        FP_rows.append(i)\n",
        "FP_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#untuk dapatkan data belum bersihnya\n",
        "data = pd.read_csv('for_preprocessingg.csv', encoding='latin-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#label dari dataset\n",
        "display(df[\"label\"].iloc[27])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#data belum preprocessing\n",
        "display(data[\"tweet\"].iloc[27])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#data bersih\n",
        "display(df[\"tweet\"].iloc[27])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing Data for LDA Modelling"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ML_Revisi_SVM_Fix.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "dc07d24e2f18896857f0b2a651fe84ba40ce7b297e58d8804a308c8039f752a6"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
